{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6TSTDsrCw4sO",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ultimatedude/dude/lib/python3.9/site-packages (4.12.3)\n",
      "Requirement already satisfied: filelock in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: requests in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (2021.11.2)\n",
      "Requirement already satisfied: sacremoses in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (0.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ultimatedude/dude/lib/python3.9/site-packages (from transformers) (21.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ultimatedude/dude/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ultimatedude/dude/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ultimatedude/dude/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ultimatedude/dude/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ultimatedude/dude/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ultimatedude/dude/lib/python3.9/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: click in /home/ultimatedude/dude/lib/python3.9/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ultimatedude/dude/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /home/ultimatedude/dude/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306 kB 377 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ultimatedude/dude/lib/python3.9/site-packages (from datasets) (21.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ultimatedude/dude/lib/python3.9/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ultimatedude/dude/lib/python3.9/site-packages (from datasets) (1.19.5)\n",
      "Collecting pyarrow!=4.0.0,>=3.0.0\n",
      "  Downloading pyarrow-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.6 MB 75 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ultimatedude/dude/lib/python3.9/site-packages (from datasets) (1.3.4)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132 kB 836 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: dill in /home/ultimatedude/dude/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ultimatedude/dude/lib/python3.9/site-packages (from datasets) (0.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ultimatedude/dude/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128 kB 1.2 MB/s            \n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp39-cp39-manylinux2010_x86_64.whl (243 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243 kB 1.5 MB/s            \n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2 MB 166 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ultimatedude/dude/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ultimatedude/dude/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /home/ultimatedude/dude/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ultimatedude/dude/lib/python3.9/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ultimatedude/dude/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ultimatedude/dude/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ultimatedude/dude/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ultimatedude/dude/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ultimatedude/dude/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304 kB 673 kB/s            \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (174 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174 kB 548 kB/s            \n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (203 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203 kB 705 kB/s            \n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ultimatedude/dude/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ultimatedude/dude/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ultimatedude/dude/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, fsspec, aiohttp, xxhash, pyarrow, multiprocess, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-1.17.0 frozenlist-1.2.0 fsspec-2021.11.1 multidict-5.2.0 multiprocess-0.70.12.2 pyarrow-6.0.1 xxhash-2.0.2 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Xpx6W3ExUXF"
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer, DataCollatorWithPadding,AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwQncjQHxnuj"
   },
   "source": [
    "## Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyR1JrUQyawI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = [\"sentence1\", \"sentence2\",\"labels\",\"similarity\"]\n",
    "data = pd.read_csv(\"actual_train.csv\", names = cols)\n",
    "data.to_csv(\"actual_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "IXlW5N3UzP_K",
    "outputId": "1f39b711-336a-4851-d0e2-1d0fd6c05d3f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'play song or music. Whatever you say'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentence2\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xu9D0pmkxgVB"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"actual_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q03nlBMByJ88",
    "outputId": "8b1f2f08-401b-49b7-cea4-57ebc8273f95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'sentence1', 'sentence2', 'labels', 'similarity'],\n",
       "        num_rows: 156\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHlSgBFozsaN"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_WafgHyyWFK"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmm2wrDAzu7t"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence1'],example[\"sentence2\"],padding = True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNr5OdA70AGq",
    "outputId": "838497cc-0d2a-4798-fa5e-4935b0b24d84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2eb4d8d6bd350683/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-c8773499dca0c249.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JC0QSOiY0Pi4"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\",\"Unnamed: 0\",\"labels\",\"similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1cLqTi40i1x"
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets = tokenized_datasets.rename_column(\"similarity\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYdgpkcp0xcH"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LG1vXMsn1pT2",
    "outputId": "5cc5384a-df57-412c-cdbb-b427faa67ae4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHIxLStdVXEm"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3u9tV1w1s1e"
   },
   "outputs": [],
   "source": [
    "# Let's define our data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_data_loader =  DataLoader(tokenized_datasets[\"train\"], shuffle = True, batch_size = 8, collate_fn = data_collator)\n",
    "# eval_data_loader = DataLoader(tokenized_datasets[\"validation\"], batch_size = 8, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_oiEfwlPcf6",
    "outputId": "8901899e-8095-4459-f31a-2fc8b186497a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peBZEaO6SMNp"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Jrja7P93fq3"
   },
   "outputs": [],
   "source": [
    "# Let's check our code\n",
    "for batch in train_data_loader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9D85Vk6C2G2J"
   },
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57FSQA9l2O9s"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5CEg1Zt2U60",
    "outputId": "a87ecdde-1634-48bf-8915-5b8566518d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "# learning rate scheduler \n",
    "# Here we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader).\n",
    "from transformers import get_scheduler\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_data_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps,\n",
    ")\n",
    "\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g74Q5KXn2XiA",
    "outputId": "485bc456-c3d5-4ecf-ccc9-ef86a8cf0141"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(250037, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Edkh9g4q4pH1"
   },
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQANCx1r2deH"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "  for batch in train_data_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLvakYJ52dtp"
   },
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "# metric = load_metric(\"glue\", \"mrpc\")\n",
    "# model.eval()\n",
    "\n",
    "# for batch in eval_data_loader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv_xsOEDX_c-"
   },
   "source": [
    "## Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UYJurLoMVioT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 20:15:05.868877: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 20:15:05.868910: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U3TGs0NkYCh6"
   },
   "outputs": [],
   "source": [
    "checkpoint = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-P_Ov06ZZXEV"
   },
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [\"sentences\",\"labels\",'encodings']\n",
    "# data = pd.read_csv(\"Command.csv\")\n",
    "# data.to_pickle(\"commands_V2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x3hWh1mGQ8vO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = [\"sentences\",\"labels\",'encodings']\n",
    "data = pd.read_pickle(\"commands_V2.pkl\")\n",
    "# data.to_csv(\"actual_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xCTW7jWeSor4",
    "outputId": "368b61fd-6f08-4f43-ea19-c3d6f9af3789"
   },
   "outputs": [],
   "source": [
    "# data = data.drop(\"Unnamed: 0\", axis = 1)\n",
    "# data.head()\n",
    "# data.to_pickle(\"commands_V2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8ytjng7zXhdH"
   },
   "outputs": [],
   "source": [
    "def get_encoding(sentence):\n",
    "  encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "  # Compute token embeddings\n",
    "  with torch.no_grad():\n",
    "      model_output = model(**encoded_input)\n",
    "  # Return encoding\n",
    "  encoding_output =  mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "  encoding_output = encoding_output.reshape(1, -1)\n",
    "  return encoding_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0       music\n",
       "1       music\n",
       "2       music\n",
       "3       music\n",
       "4       music\n",
       "        ...  \n",
       "101    reboot\n",
       "102      date\n",
       "103      date\n",
       "104      date\n",
       "105     intro\n",
       "Name: labels, Length: 106, dtype: object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels'].unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add new sentence here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_sen(sen,data):\n",
    "    enc = get_encoding(sen)\n",
    "    label = input(\"Enter the label: \")\n",
    "#     df2 = {'sentences': sen, 'labels': label, 'encodings': enc}\n",
    "    df2 = pd.DataFrame({'sentences': [sen], 'labels': [label], 'encodings': [enc]})\n",
    "    data = pd.concat([data,df2], ignore_index = True, axis = 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: Launch Firefox browser\n",
      "Enter the label: firefox\n",
      "Enter a sentence: bring out my firefox\n",
      "Enter the label: firefox\n",
      "Enter a sentence: I need firefox\n",
      "Enter the label: firefox\n",
      "Enter a sentence: Launch a browser\n",
      "Enter the label: browser\n",
      "Enter a sentence: I need a browser\n",
      "Enter the label: browser\n",
      "Enter a sentence: done\n"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    sen = input(\"Enter a sentence: \")\n",
    "    if sen == \"done\":\n",
    "        break\n",
    "    data = add_new_sen(sen,data)\n",
    "    data.to_pickle(\"commands_V2.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Commmand.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# cols = [\"sentences\",\"labels\",'encodings']\n",
    "# data = pd.read_pickle(\"commands_V2.pkl\")\n",
    "# # data.to_csv(\"actual_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eoYARRQ3eFNl"
   },
   "outputs": [],
   "source": [
    "def similarity_score(sentence):\n",
    "  encoded_output = get_encoding(sentence)\n",
    "  scores = []\n",
    "  min_score = 0.68\n",
    "  for i in range(len(data)):\n",
    "    score = cosine_similarity(data[\"encodings\"][i], encoded_output)\n",
    "    scores.append(score)\n",
    "    command_idx = np.argmax(scores)\n",
    "    max_score = np.max(scores)\n",
    "    \n",
    "#     if max_score < min_score:\n",
    "#         label = \"web search\"\n",
    "#     else:\n",
    "    label = data[\"labels\"][command_idx]\n",
    "        \n",
    "  return command_idx,label,max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36YecTuieFQc",
    "outputId": "9072066c-e3f2-424d-a163-76852840ecb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 'firefox', 0.8275683)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"Launch firefox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ohh yeh.....ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±<br>\n",
    "The Primordial dude you are Great....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "MYeT5H1mqqyx"
   },
   "outputs": [],
   "source": [
    "data.to_pickle(\"commands.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaznZaBVeKWy"
   },
   "outputs": [],
   "source": [
    "data[\"sentences\"][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iRIsoa4d6uz"
   },
   "source": [
    "## Test Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orWP1V3HU1xT",
    "outputId": "0a5bd964-b35d-44a3-d0f1-40ae08f92dec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open skype', 'open google meet']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences = [data[\"sentence1\"][27], data[\"sentence1\"][30]]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>labels</th>\n",
       "      <th>encodings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>open music</td>\n",
       "      <td>music</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paly music for me</td>\n",
       "      <td>music</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can you play anything? my friends are getting ...</td>\n",
       "      <td>music</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>choose a song from library and play it</td>\n",
       "      <td>music</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am bored,can you play a song for me</td>\n",
       "      <td>music</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences labels encodings\n",
       "0                                         open music  music         D\n",
       "1                                  paly music for me  music       NaN\n",
       "2  can you play anything? my friends are getting ...  music       NaN\n",
       "3             choose a song from library and play it  music       NaN\n",
       "4              i am bored,can you play a song for me  music       NaN"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cols = [\"sentences\",\"labels\", 'temp','encodings']\n",
    "data = pd.read_csv(\"commands(Product).csv\", names=cols)\n",
    "data = data.drop(\"temp\", axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'launch settings'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentences\"][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "j8Wgr7hQT5fK"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[43msentences\u001b[49m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-n3QIchm2dzC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Perform pooling. In this case, max pooling.\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5eyvJckVAk5"
   },
   "outputs": [],
   "source": [
    "# print(\"Sentence embeddings:\")\n",
    "# print(sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2t8Yz5B0Xuja"
   },
   "outputs": [],
   "source": [
    "# v1 = sentence_embeddings[0].reshape(1, -1)\n",
    "# v2 = sentence_embeddings[1].reshape(1, -1)\n",
    "# cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Td_SoxZwU_MX",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done0\n",
      "done1\n",
      "done2\n",
      "done3\n",
      "done4\n",
      "done5\n",
      "done6\n",
      "done7\n",
      "done8\n",
      "done9\n",
      "done10\n",
      "done11\n",
      "done12\n",
      "done13\n",
      "done14\n",
      "done15\n",
      "done16\n",
      "done17\n",
      "done18\n",
      "done19\n",
      "done20\n",
      "done21\n",
      "done22\n",
      "done23\n",
      "done24\n",
      "done25\n",
      "done26\n",
      "done27\n",
      "done28\n",
      "done29\n",
      "done30\n",
      "done31\n",
      "done32\n",
      "done33\n",
      "done34\n",
      "done35\n",
      "done36\n",
      "done37\n",
      "done38\n",
      "done39\n",
      "done40\n",
      "done41\n",
      "done42\n",
      "done43\n",
      "done44\n",
      "done45\n",
      "done46\n",
      "done47\n",
      "done48\n",
      "done49\n",
      "done50\n",
      "done51\n",
      "done52\n",
      "done53\n",
      "done54\n",
      "done55\n",
      "done56\n",
      "done57\n",
      "done58\n",
      "done59\n",
      "done60\n",
      "done61\n",
      "done62\n",
      "done63\n",
      "done64\n",
      "done65\n",
      "done66\n",
      "done67\n",
      "done68\n",
      "done69\n",
      "done70\n",
      "done71\n",
      "done72\n",
      "done73\n",
      "done74\n",
      "done75\n",
      "done76\n",
      "done77\n",
      "done78\n",
      "done79\n",
      "done80\n",
      "done81\n",
      "done82\n",
      "done83\n",
      "done84\n",
      "done85\n",
      "done86\n",
      "done87\n",
      "done88\n",
      "done89\n",
      "done90\n",
      "done91\n",
      "done92\n",
      "done93\n",
      "done94\n",
      "done95\n",
      "done96\n",
      "done97\n",
      "done98\n",
      "done99\n",
      "done100\n",
      "done101\n",
      "done102\n",
      "done103\n",
      "done104\n",
      "done105\n",
      "done106\n",
      "done107\n",
      "done108\n",
      "done109\n",
      "done110\n",
      "done111\n",
      "done112\n",
      "done113\n",
      "done114\n",
      "done115\n",
      "done116\n",
      "done117\n",
      "done118\n",
      "done119\n",
      "done120\n",
      "done121\n",
      "done122\n",
      "done123\n",
      "done124\n",
      "done125\n",
      "done126\n",
      "done127\n",
      "done128\n",
      "done129\n",
      "done130\n",
      "done131\n",
      "done132\n",
      "done133\n",
      "done134\n",
      "done135\n",
      "done136\n",
      "done137\n",
      "done138\n",
      "done139\n",
      "done140\n",
      "done141\n"
     ]
    }
   ],
   "source": [
    "# encoding = get_encoding(data[\"sentences\"][1])\n",
    "lst = []\n",
    "for i in range(len(data)):\n",
    "  encoding = get_encoding(data[\"sentences\"][i])\n",
    "  print(f\"done{i}\")\n",
    "  lst.append(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTCjpJf6VCg1",
    "outputId": "8721fab7-b939-4150-e3e0-02c1408f145e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[\"encodings\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RfH7C4C7ZbBf",
    "outputId": "1cb8ae6f-2261-4398-9db8-86f2b2a22f4f"
   },
   "outputs": [],
   "source": [
    "data[\"encodings\"] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"commands_V2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "XPc8TVE3Zesf"
   },
   "outputs": [],
   "source": [
    "# data.to_csv(\"commands.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRDAGEhlZvCR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FwQncjQHxnuj",
    "vHlSgBFozsaN",
    "bHIxLStdVXEm",
    "0iRIsoa4d6uz"
   ],
   "name": "Alice_Command_Similarity_Pytorch_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
