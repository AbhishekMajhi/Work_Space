{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "convolutional-neural-networks",
      "graded_item_id": "OMdut",
      "launcher_item_id": "bbBOL"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq6qfbGvrNon"
      },
      "source": [
        "# Autonomous driving - Car detection\n",
        "Ideas in this notebook are from the two YOLO papers: [Redmon et al., 2016](https://arxiv.org/abs/1506.02640) and [Redmon and Farhadi, 2016](https://arxiv.org/abs/1612.08242).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg6k5wc87SSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee74427d-bf53-4e00-fc0e-656c20295f45"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOiBHU97753I"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZhtRJKO78q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6677d62c-4ae0-4a09-fe0e-a98edd7d7bf8"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/Datasets/Autonomous_car_driving/Car detection for Autonomous Driving')\n",
        "#os.chdir('Car detection for Autonomous Driving')\n",
        "#os.getcwd()\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yolov4.weights',\n",
              " 'Autonomous_driving_application_Car_detection_v3a.ipynb',\n",
              " 'Drive.ai Dataset Sample LICENSE',\n",
              " 'LICENSE',\n",
              " 'font',\n",
              " 'images',\n",
              " 'model_data',\n",
              " 'nb_images',\n",
              " 'out',\n",
              " 'yad2k',\n",
              " '__pycache__',\n",
              " '.ipynb_checkpoints',\n",
              " 'yolo_utils.py',\n",
              " 'yolov2.weights']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U56xCY7tuuv"
      },
      "source": [
        "**Downloading YOLOv2 weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTNeX-QzspE1",
        "outputId": "3b372f93-0b0a-4954-aec3-63701f37fcd8"
      },
      "source": [
        "!wget 'https://pjreddie.com/media/files/yolov2.weights'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-07 02:42:39--  https://pjreddie.com/media/files/yolov2.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203934260 (194M) [application/octet-stream]\n",
            "Saving to: ‘yolov2.weights’\n",
            "\n",
            "yolov2.weights      100%[===================>] 194.49M  73.0MB/s    in 2.7s    \n",
            "\n",
            "2021-05-07 02:42:41 (73.0 MB/s) - ‘yolov2.weights’ saved [203934260/203934260]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYDZhi8NzJ8T"
      },
      "source": [
        "#pip install --upgrade tensorflow-gpu==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzsvHIcTzKAI"
      },
      "source": [
        "#pip install --upgrade keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKHuP21KlovQ"
      },
      "source": [
        "**Downloading YOLOv4 pretrained weights..**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cZb5SUYln7q"
      },
      "source": [
        "#!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqDdfrsr8LQV"
      },
      "source": [
        "Now we are in proper dir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DvtkuC08Cqq"
      },
      "source": [
        "#! git clone 'https://github.com/amanchadha/coursera-deep-learning-specialization.git'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nQikTD32qQ4"
      },
      "source": [
        "# import colorsys\n",
        "# import imghdr\n",
        "# import os\n",
        "# import random\n",
        "# from keras import backend as K\n",
        "\n",
        "# import numpy as np\n",
        "# from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# def read_classes(classes_path):\n",
        "#     with open(classes_path) as f:\n",
        "#         class_names = f.readlines()\n",
        "#     class_names = [c.strip() for c in class_names]\n",
        "#     return class_names\n",
        "\n",
        "# def read_anchors(anchors_path):\n",
        "#     with open(anchors_path) as f:\n",
        "#         anchors = f.readline()\n",
        "#         try:\n",
        "#           anchors = [float(x) for x in anchors.split(',')]\n",
        "#         except ValueError as e:\n",
        "#           print (\"error\",e,\"on line\")\n",
        "#         anchors = np.array(anchors).reshape(-1, 2)\n",
        "#     return anchors\n",
        "\n",
        "# def generate_colors(class_names):\n",
        "#     hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n",
        "#     colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "#     colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "#     random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
        "#     random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
        "#     random.seed(None)  # Reset seed to default.\n",
        "#     return colors\n",
        "\n",
        "# def scale_boxes(boxes, image_shape):\n",
        "#     \"\"\" Scales the predicted boxes in order to be drawable on the image\"\"\"\n",
        "#     height = image_shape[0]\n",
        "#     width = image_shape[1]\n",
        "#     image_dims = K.stack([height, width, height, width])\n",
        "#     image_dims = K.reshape(image_dims, [1, 4]).toString()\n",
        "#     boxes = boxes * image_dims\n",
        "#     return boxes\n",
        "\n",
        "# def preprocess_image(img_path, model_image_size):\n",
        "#     image_type = imghdr.what(img_path)\n",
        "#     image = Image.open(img_path)\n",
        "#     resized_image = image.resize(tuple(reversed(model_image_size)), Image.BICUBIC)\n",
        "#     image_data = np.array(resized_image, dtype='float32')\n",
        "#     image_data /= 255.\n",
        "#     image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "#     return image, image_data\n",
        "\n",
        "# def draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors):\n",
        "\n",
        "#     font = ImageFont.truetype(font='font/FiraMono-Medium.otf',size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
        "#     thickness = (image.size[0] + image.size[1]) // 300\n",
        "\n",
        "#     for i, c in reversed(list(enumerate(out_classes))):\n",
        "#         predicted_class = class_names[c]\n",
        "#         box = out_boxes[i]\n",
        "#         score = out_scores[i]\n",
        "\n",
        "#         label = '{} {:.2f}'.format(predicted_class, score)\n",
        "\n",
        "#         draw = ImageDraw.Draw(image)\n",
        "#         label_size = draw.textsize(label, font)\n",
        "\n",
        "#         top, left, bottom, right = box\n",
        "#         top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "#         left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "#         bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
        "#         right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
        "#         print(label, (left, top), (right, bottom))\n",
        "\n",
        "#         if top - label_size[1] >= 0:\n",
        "#             text_origin = np.array([left, top - label_size[1]])\n",
        "#         else:\n",
        "#             text_origin = np.array([left, top + 1])\n",
        "\n",
        "#         # My kingdom for a good redistributable image drawing library.\n",
        "#         for i in range(thickness):\n",
        "#             draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
        "#         draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=colors[c])\n",
        "#         draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "#         del draw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRtSMQ35rNop"
      },
      "source": [
        "## Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFAatXf4rNoq"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Lambda, Conv2D\n",
        "from keras.models import load_model, Model\n",
        "from yolo_utils import read_classes, read_anchors, generate_colors, draw_boxes, scale_boxes,preprocess_image\n",
        "from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS3hOu3RrNo5"
      },
      "source": [
        "# yolo filter boxes\n",
        "\n",
        "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):\n",
        "    \"\"\"Filters YOLO boxes by thresholding on object and class confidence.\n",
        "\n",
        "    Arguments:\n",
        "    box_confidence -- tensor of shape (19, 19, 5, 1)\n",
        "    boxes -- tensor of shape (19, 19, 5, 4)\n",
        "    box_class_probs -- tensor of shape (19, 19, 5, 80)\n",
        "    threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n",
        "\n",
        "    Returns:\n",
        "    scores -- tensor of shape (None,), containing the class probability score for selected boxes\n",
        "    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes\n",
        "    classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute box scores\n",
        "    box_scores = box_confidence * box_class_probs\n",
        "\n",
        "\n",
        "    #  Finding the box_classes using the max box_scores\n",
        "    box_classes = K.argmax(box_scores, axis = -1) # class having maximum box score\n",
        "    box_class_scores = K.max(box_scores, axis= -1) # finding the box score of that max one.\n",
        "\n",
        "\n",
        "    # Creating a filtering mask based on \"box_class_scores\" by using \"threshold\". The mask should have the\n",
        "    # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability >= threshold)\n",
        "    filtering_mask = (box_class_scores >= threshold)\n",
        "\n",
        "\n",
        "    # Applying the mask to box_class_scores, boxes and box_classes\n",
        "    scores = tf.boolean_mask(box_class_scores, filtering_mask)\n",
        "    boxes = tf.boolean_mask(boxes, filtering_mask)\n",
        "    classes = tf.boolean_mask(box_classes, filtering_mask)\n",
        "\n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNBxZ_LBd_o_",
        "outputId": "2528ec3f-3319-4645-80c9-64ae170bc2c4"
      },
      "source": [
        "with tf.Session() as test_a:\n",
        "    box_confidence = tf.random_normal([19, 19, 5, 1], mean=1, stddev=4, seed = 1)\n",
        "    boxes = tf.random_normal([19, 19, 5, 4], mean=1, stddev=4, seed = 1)\n",
        "    box_class_probs = tf.random_normal([19, 19, 5, 80], mean=1, stddev=4, seed = 1)\n",
        "    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = 0.5)\n",
        "    print(\"scores[2] = \" + str(scores[2].eval()))\n",
        "    print(\"boxes[2] = \" + str(boxes[2].eval()))\n",
        "    print(\"classes[2] = \" + str(classes[2].eval()))\n",
        "    print(\"scores.shape = \" + str(scores.shape))\n",
        "    print(\"boxes.shape = \" + str(boxes.shape))\n",
        "    print(\"classes.shape = \" + str(classes.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scores[2] = 10.750582\n",
            "boxes[2] = [ 8.426533   3.2713668 -0.5313436 -4.9413733]\n",
            "classes[2] = 7\n",
            "scores.shape = (?,)\n",
            "boxes.shape = (?, 4)\n",
            "classes.shape = (?,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tl5X1pXrNpF"
      },
      "source": [
        "# Calculating iou(IoU) between box1 and box2\n",
        "\n",
        "\n",
        "def iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    box1 -- first box, list object with coordinates (box1_x1, box1_y1, box1_x2, box_1_y2)\n",
        "    box2 -- second box, list object with coordinates (box2_x1, box2_y1, box2_x2, box2_y2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Assigning variable names to coordinates for clarity\n",
        "    (box1_x1, box1_y1, box1_x2, box1_y2) = box1\n",
        "    (box2_x1, box2_y1, box2_x2, box2_y2) = box2\n",
        "    #print(box1_x1, box1_y1, box1_x2, box1_y2)\n",
        "    #print(box2_x1, box2_y1, box2_x2, box2_y2)\n",
        "\n",
        "    # Calculating the (yi1, xi1, yi2, xi2) coordinates of the intersection of box1 and box2. Calculate its Area.\n",
        "    xi1 = max(box1[0], box2[0])\n",
        "    yi1 = max(box1[1], box2[1])\n",
        "    xi2 = min(box1[2], box2[2])\n",
        "    yi2 = min(box1[3], box2[3])\n",
        "\n",
        "\n",
        "    inter_width = (xi2 - xi1)\n",
        "    inter_height = (yi2 - yi1)\n",
        "    inter_area = np.maximum((xi2 - xi1), 0)*np.maximum((yi2 - yi1),0)\n",
        "    print(\"inter_area:\", inter_area)\n",
        "\n",
        "\n",
        "    # Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
        "    box1_area = (box1_y2 - box1_y1) * (box1_x2 - box1_x1)\n",
        "    box2_area = (box2_y2 - box2_y1) * (box2_x2 - box2_x1)\n",
        "    union_area = (box1_area + box2_area) - inter_area\n",
        "\n",
        "    # compute the IoU\n",
        "    iou = inter_area / union_area\n",
        "\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqvu9Owten0y",
        "outputId": "14b44930-f94c-4d5a-a659-eefa2edb2f20"
      },
      "source": [
        "## Test case 1: boxes intersect\n",
        "box1 = (2, 1, 4, 3)\n",
        "box2 = (1, 2, 3, 4)\n",
        "print(\"iou for intersecting boxes = \" + str(iou(box1, box2)))\n",
        "\n",
        "## Test case 2: boxes do not intersect\n",
        "box1 = (1,2,3,4)\n",
        "box2 = (5,6,7,8)\n",
        "print(\"iou for non-intersecting boxes = \" + str(iou(box1,box2)))\n",
        "\n",
        "## Test case 3: boxes intersect at vertices only\n",
        "box1 = (1,1,2,2)\n",
        "box2 = (2,2,3,3)\n",
        "print(\"iou for boxes that only touch at vertices = \" + str(iou(box1,box2)))\n",
        "\n",
        "## Test case 4: boxes intersect at edge only\n",
        "box1 = (1,1,3,3)\n",
        "box2 = (2,3,3,4)\n",
        "print(\"iou for boxes that only touch at edges = \" + str(iou(box1,box2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inter_area: 1\n",
            "iou for intersecting boxes = 0.14285714285714285\n",
            "inter_area: 0\n",
            "iou for non-intersecting boxes = 0.0\n",
            "inter_area: 0\n",
            "iou for boxes that only touch at vertices = 0.0\n",
            "inter_area: 0\n",
            "iou for boxes that only touch at edges = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVek5YLurNpQ"
      },
      "source": [
        "# Applying yolo_non_max_suppression to set of boxes\n",
        "\n",
        "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    scores -- tensor of shape (None,), output of yolo_filter_boxes()\n",
        "    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)\n",
        "    classes -- tensor of shape (None,), output of yolo_filter_boxes()\n",
        "    max_boxes -- integer, maximum number of predicted boxes you'd like\n",
        "    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n",
        "\n",
        "    Returns:\n",
        "    scores -- tensor of shape (, None), predicted score for each box\n",
        "    boxes -- tensor of shape (4, None), predicted box coordinates\n",
        "    classes -- tensor of shape (, None), predicted class for each box\n",
        "    \"\"\"\n",
        "\n",
        "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')     # tensor to be used in tf.image.non_max_suppression()\n",
        "    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor\n",
        "\n",
        "\n",
        "    nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes_tensor, iou_threshold=iou_threshold)\n",
        "\n",
        "    scores = K.gather(scores, nms_indices)\n",
        "    boxes = K.gather(boxes, nms_indices)\n",
        "    classes = K.gather(classes, nms_indices)\n",
        "\n",
        "\n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F62uKvZUewm8",
        "outputId": "bcaa38b1-532c-4e85-94e6-5f92cfe0eeba"
      },
      "source": [
        "with tf.Session() as test_b:\n",
        "    scores = tf.random_normal([54,], mean=1, stddev=4, seed = 1)\n",
        "    boxes = tf.random_normal([54, 4], mean=1, stddev=4, seed = 1)\n",
        "    classes = tf.random_normal([54,], mean=1, stddev=4, seed = 1)\n",
        "    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes)\n",
        "    print(\"scores[2] = \" + str(scores[2].eval()))\n",
        "    print(\"boxes[2] = \" + str(boxes[2].eval()))\n",
        "    print(\"classes[2] = \" + str(classes[2].eval()))\n",
        "    print(\"scores.shape = \" + str(scores.eval().shape))\n",
        "    print(\"boxes.shape = \" + str(boxes.eval().shape))\n",
        "    print(\"classes.shape = \" + str(classes.eval().shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scores[2] = 6.938395\n",
            "boxes[2] = [-5.299932    3.1379814   4.450367    0.95942086]\n",
            "classes[2] = -2.2452729\n",
            "scores.shape = (10,)\n",
            "boxes.shape = (10, 4)\n",
            "classes.shape = (10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aSgMU9urNpa"
      },
      "source": [
        "# yolo eval\n",
        "\n",
        "def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:\n",
        "                    box_confidence: tensor of shape (None, 19, 19, 5, 1)\n",
        "                    box_xy: tensor of shape (None, 19, 19, 5, 2)\n",
        "                    box_wh: tensor of shape (None, 19, 19, 5, 2)\n",
        "                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)\n",
        "    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)\n",
        "    max_boxes -- integer, maximum number of predicted boxes you'd like\n",
        "    score_threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n",
        "    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n",
        "\n",
        "    Returns:\n",
        "    scores -- tensor of shape (None, ), predicted score for each box\n",
        "    boxes -- tensor of shape (None, 4), predicted box coordinates\n",
        "    classes -- tensor of shape (None,), predicted class for each box\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve outputs of the YOLO model\n",
        "    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs\n",
        "\n",
        "    # Converting boxes to be ready for filtering functions (convert boxes box_xy and box_wh to corner coordinates)\n",
        "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
        "\n",
        "    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = score_threshold)\n",
        "\n",
        "    # Scale boxes back to original image shape.\n",
        "    boxes = scale_boxes(boxes, image_shape)\n",
        "\n",
        "    # Use one of the functions you've implemented to perform Non-max suppression with\n",
        "    # maximum number of boxes set to max_boxes and a threshold of iou_threshold\n",
        "    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes,iou_threshold)\n",
        "\n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-yeRtYoe_Yj",
        "outputId": "aafb899f-e533-4126-8b35-f1d078d199ff"
      },
      "source": [
        "with tf.Session() as test_b:\n",
        "    yolo_outputs = (tf.random_normal([19, 19, 5, 1], mean=1, stddev=4, seed = 1),\n",
        "                    tf.random_normal([19, 19, 5, 2], mean=1, stddev=4, seed = 1),\n",
        "                    tf.random_normal([19, 19, 5, 2], mean=1, stddev=4, seed = 1),\n",
        "                    tf.random_normal([19, 19, 5, 80], mean=1, stddev=4, seed = 1))\n",
        "    scores, boxes, classes = yolo_eval(yolo_outputs)\n",
        "    print(\"scores[2] = \" + str(scores[2].eval()))\n",
        "    print(\"boxes[2] = \" + str(boxes[2].eval()))\n",
        "    print(\"classes[2] = \" + str(classes[2].eval()))\n",
        "    print(\"scores.shape = \" + str(scores.eval().shape))\n",
        "    print(\"boxes.shape = \" + str(boxes.eval().shape))\n",
        "    print(\"classes.shape = \" + str(classes.eval().shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scores[2] = 138.79124\n",
            "boxes[2] = [1292.3297  -278.52167 3876.9893  -835.56494]\n",
            "classes[2] = 54\n",
            "scores.shape = (10,)\n",
            "boxes.shape = (10, 4)\n",
            "classes.shape = (10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYxrARfbrNpl"
      },
      "source": [
        "##  Test YOLO pre-trained model on images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-U1nmCJrNpl"
      },
      "source": [
        "Here we gonna use a pre-trained model and test it on the car detection dataset.  We'll need a session to execute the computation graph and evaluate the tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DKyYgDnrNpm"
      },
      "source": [
        "sess = K.get_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_qrqxHouM_J"
      },
      "source": [
        "**Doing some stuffs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6XGxdBtuMW8"
      },
      "source": [
        "# # importing required modules\n",
        "# from zipfile import ZipFile\n",
        "\n",
        "# # specifying the zip file name\n",
        "# file_name = \"/content/drive/MyDrive/Datasets/Autonomous_car_driving/Car detection for Autonomous Driving/yolo.h5.zip\"\n",
        "\n",
        "# # opening the zip file in READ mode\n",
        "# with ZipFile(file_name, 'r') as zip:\n",
        "#     # printing all the contents of the zip file\n",
        "#     zip.printdir()\n",
        "\n",
        "#     # extracting all the files\n",
        "#     print('Extracting all the files now...')\n",
        "#     zip.extractall()\n",
        "#     print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg3jsxQ7wysT"
      },
      "source": [
        "# def fixBadZipfile(zipFile):\n",
        "#   f = open(zipFile, 'r+b')\n",
        "#   data = f.read()\n",
        "#   pos = data.find('\\x50\\x4b\\x05\\x06') # End of central directory signature\n",
        "#   if (pos > 0):\n",
        "#       self._log(\"Trancating file at location \" + str(pos + 22)+ \".\")\n",
        "#       f.seek(pos + 22)   # size of 'ZIP end of central directory record'\n",
        "#       f.truncate()\n",
        "#       f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2OHGidDrNpr"
      },
      "source": [
        "class_names = read_classes(\"model_data/coco_classes.txt\")\n",
        "anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
        "image_shape = (720., 1280.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH5VL5GPrNpu"
      },
      "source": [
        "###  Loading a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd7sZ3nVrNpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25120899-fcc4-488e-939d-b3104c41d51c"
      },
      "source": [
        "yolo_model = load_model(\"model_data/yolo.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiJjuNjVzc9s"
      },
      "source": [
        "import keras\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "YWir6mMJzOby",
        "outputId": "2af12d4f-5b6c-461b-d1b2-ce23f8e03de8"
      },
      "source": [
        "yolo_model.compile(loss='categorical_crossentropy', optimizer=opt,  weighted_metrics= '/content/drive/MyDrive/Datasets/Autonomous_car_driving/Car detection for Autonomous Driving/yolov2.weights')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-13779521dbb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myolo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Datasets/Autonomous_car_driving/Car detection for Autonomous Driving/yolov2.weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Save all metric attributes per output of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_output_metric_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# Set metric attributes on model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_cache_output_metric_attributes\u001b[0;34m(self, metrics, weighted_metrics)\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 is_weighted=True))\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_unique_metric_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcollect_per_output_metric_info\u001b[0;34m(metrics, output_names, output_shapes, loss_fns, is_weighted)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         raise TypeError('Type of `metrics` argument not understood. '\n\u001b[0;32m--> 935\u001b[0;31m                         'Expected a list or dictionary, found: ' + str(metrics))\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0mper_output_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Type of `metrics` argument not understood. Expected a list or dictionary, found: /content/drive/MyDrive/Datasets/Autonomous_car_driving/Car detection for Autonomous Driving/yolov2.weights"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b5dAyIhrNp0"
      },
      "source": [
        "This loads the weights of a trained YOLO model. Here's a summary of the layers your model contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQH5OkxgrNp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff6bbd5-d761-43c6-a5e5-e7127407074c"
      },
      "source": [
        "yolo_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 608, 608, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 608, 608, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 608, 608, 32) 128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 608, 608, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 304, 304, 32) 0           leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 304, 304, 64) 18432       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 304, 304, 64) 256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 304, 304, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 152, 152, 64) 0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 152, 152, 128 73728       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 152, 152, 128 512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 152, 152, 128 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 152, 152, 64) 8192        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 152, 152, 64) 256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 152, 152, 64) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 152, 152, 128 73728       leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 152, 152, 128 512         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 152, 152, 128 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 76, 76, 128)  0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 76, 76, 256)  294912      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 76, 76, 256)  1024        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 76, 76, 256)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 76, 76, 128)  32768       leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 76, 76, 128)  512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 76, 76, 128)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 76, 76, 256)  294912      leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 76, 76, 256)  1024        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 76, 76, 256)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 38, 38, 256)  0           leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 38, 38, 512)  1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 38, 38, 512)  2048        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 38, 38, 512)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 38, 38, 256)  131072      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 38, 38, 256)  1024        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 38, 38, 256)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 38, 38, 512)  1179648     leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 38, 38, 512)  2048        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 38, 38, 512)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 38, 38, 256)  131072      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 38, 38, 256)  1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 38, 38, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 38, 38, 512)  1179648     leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 38, 38, 512)  2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 38, 38, 512)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 19, 19, 512)  0           leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 19, 19, 1024) 4718592     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 19, 19, 1024) 4096        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 19, 19, 512)  524288      leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 19, 19, 512)  2048        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 19, 19, 512)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 19, 19, 1024) 4718592     leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 19, 19, 1024) 4096        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 19, 19, 512)  524288      leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 19, 19, 512)  2048        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, 19, 19, 512)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 19, 19, 1024) 4718592     leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 19, 19, 1024) 4096        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 19, 19, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 19, 19, 1024) 4096        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 38, 38, 64)   32768       leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 38, 38, 64)   256         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 19, 19, 1024) 9437184     leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 38, 38, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 19, 19, 1024) 4096        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "space_to_depth_x2 (Lambda)      (None, 19, 19, 256)  0           leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 19, 19, 1280) 0           space_to_depth_x2[0][0]          \n",
            "                                                                 leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 19, 19, 1024) 11796480    concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 19, 19, 1024) 4096        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 19, 19, 425)  435625      leaky_re_lu_22[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 50,983,561\n",
            "Trainable params: 50,962,889\n",
            "Non-trainable params: 20,672\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qD-jT81rNp4"
      },
      "source": [
        "###  Convert output of the model to usable bounding box tensors\n",
        "\n",
        "The output of `yolo_model` is a (m, 19, 19, 5, 85) tensor that needs to pass through non-trivial processing and conversion.\n",
        "\n",
        " `yolo_head` implementation can find the function definition in the file ['keras_yolo.py'](https://github.com/allanzelener/YAD2K/blob/master/yad2k/models/keras_yolo.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO1SXhahrNp5"
      },
      "source": [
        "yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJNX2KvGrNp9"
      },
      "source": [
        "###  Filtering boxes\n",
        "\n",
        "`yolo_outputs` gave us all the predicted boxes of `yolo_model` in the correct format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kjyNKB8rNp9"
      },
      "source": [
        "scores, boxes, classes = yolo_eval(yolo_outputs, image_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4VyIkN6rNqB"
      },
      "source": [
        "### Running the graph on an image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zey7eLTmrNqB"
      },
      "source": [
        "def predict(sess, image_file):\n",
        "    \"\"\"\n",
        "    Runs the graph stored in \"sess\" to predict boxes for \"image_file\". Prints and plots the predictions.\n",
        "\n",
        "    Arguments:\n",
        "    sess -- your tensorflow/Keras session containing the YOLO graph\n",
        "    image_file -- name of an image stored in the \"images\" folder.\n",
        "\n",
        "    Returns:\n",
        "    out_scores -- tensor of shape (None, ), scores of the predicted boxes\n",
        "    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes\n",
        "    out_classes -- tensor of shape (None, ), class index of the predicted boxes\n",
        "\n",
        "    Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess your image\n",
        "    print('line 1')\n",
        "    image, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (608, 608))\n",
        "    print(image_data.shape)\n",
        "\n",
        "    # Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n",
        "    # You'll need to use feed_dict={yolo_model.input: ... , K.learning_phase(): 0})\n",
        "    ### START CODE HERE ### (≈ 1 line)\n",
        "    print('line 2')\n",
        "    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes], feed_dict={yolo_model.input: image_data, K.learning_phase():0})\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print predictions info\n",
        "    print('Found {} boxes for {}'.format(len(out_boxes), image_file))\n",
        "    # Generate colors for drawing bounding boxes.\n",
        "    colors = generate_colors(class_names)\n",
        "    # Draw bounding boxes on the image file\n",
        "    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)\n",
        "    # Save the predicted bounding box on the image\n",
        "    image.save(os.path.join(\"out\", image_file), quality=90)\n",
        "    # Display the results in the notebook\n",
        "    output_image = scipy.misc.imread(os.path.join(\"out\", image_file))\n",
        "    imshow(output_image)\n",
        "\n",
        "    return out_scores, out_boxes, out_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "UxTX41MfrNqF"
      },
      "source": [
        "out_scores, out_boxes, out_classes = predict(sess, \"test.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsgHIVEMrNqM"
      },
      "source": [
        "**References**: The ideas presented in this notebook came primarily from the two YOLO papers. The implementation here also took significant inspiration and used many components from Allan Zelener's GitHub repository. The pre-trained weights used in this exercise came from the official YOLO website.\n",
        "- Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (2015)\n",
        "- Joseph Redmon, Ali Farhadi - [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) (2016)\n",
        "- Allan Zelener - [YAD2K: Yet Another Darknet 2 Keras](https://github.com/allanzelener/YAD2K)\n",
        "- The official YOLO website (https://pjreddie.com/darknet/yolo/)"
      ]
    }
  ]
}