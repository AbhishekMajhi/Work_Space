{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1ICnIZTLKnF"
   },
   "source": [
    "# Image Captioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOsaYYPvd8-j"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENmYOWXId_rX",
    "outputId": "c6e1a9b1-2136-457d-fff7-2a5b8154b41f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg-qCJiALKnN"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfnWlucxLKnO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import efficientnet\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "seed = 111\n",
    "np.random.seed(seed)  # setting numpy random seed\n",
    "tf.random.set_seed(seed)  # settig tensorflow seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjM9DrFLLKnR"
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "We will be using the Flickr8K dataset for this tutorial. This dataset comprises over\n",
    "8,000 images, that are each paired with five different captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MyUgHgFLKnS"
   },
   "outputs": [],
   "source": [
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
    "!unzip -qq Flickr8k_Dataset.zip\n",
    "!unzip -qq Flickr8k_text.zip\n",
    "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwT-GfVzLKnU"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path to the images\n",
    "IMAGES_PATH = \"Flicker8k_Dataset\"\n",
    "\n",
    "# Desired image dimensions/ resolution\n",
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "# Vocabulary size\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Fixed length allowed for any sequence\n",
    "SEQ_LENGTH = 26\n",
    "\n",
    "# Dimension for the image embeddings and token embeddings(embedding dimension)\n",
    "EMBED_DIM = 512\n",
    "\n",
    "# Per-layer units in the feed-forward network\n",
    "FF_DIM = 512\n",
    "\n",
    "# Other training parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 60\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j45lmmzZQIOQ"
   },
   "source": [
    "<h3><b>Note<b></h3>\n",
    "<b>tf.data.AUTOTUNE<b> prompt the tf. data runtime to tune the value dynamically at runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnqTY1ZKLKnV"
   },
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKbcdyi2SHLl"
   },
   "outputs": [],
   "source": [
    "# s = 'Now is better than never. \\n'\n",
    "# print(s)\n",
    "# print(len(s))\n",
    "# new_s = s.rstrip()\n",
    "# print(new_s)\n",
    "# print(len(new_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnOJ16FWLKnW",
    "outputId": "be3247a9-8852-47b0-8e6e-d07b66b2e748"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_captions_data(filename):\n",
    "    \"\"\"Loads captions (text) data and maps them to corresponding images.\n",
    "\n",
    "    Args:\n",
    "        filename: Path to the text file containing caption data.\n",
    "\n",
    "    Returns:\n",
    "        caption_mapping: Dictionary mapping image names and the corresponding captions\n",
    "        text_data: List containing all the available captions\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename) as caption_file:\n",
    "        caption_data = caption_file.readlines() # Read the captions\n",
    "        caption_mapping = {}\n",
    "        text_data = []\n",
    "        images_to_skip = set()\n",
    "\n",
    "        for line in caption_data:   # Go through each captions\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            # Image name and captions are separated using a tab\n",
    "            img_name, caption = line.split(\"\\t\")\n",
    "\n",
    "            # Each image is repeated five times for the five different captions.\n",
    "            # Each image name has a suffix `#(caption_number)`\n",
    "            img_name = img_name.split(\"#\")[0]\n",
    "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
    "\n",
    "            # We will remove caption that are either too short to too long\n",
    "            tokens = caption.strip().split()\n",
    "\n",
    "            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n",
    "                images_to_skip.add(img_name)\n",
    "                continue\n",
    "\n",
    "            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n",
    "                # We will add a start and an end token to each caption\n",
    "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
    "                text_data.append(caption)\n",
    "\n",
    "                if img_name in caption_mapping:\n",
    "                    caption_mapping[img_name].append(caption)\n",
    "                else:\n",
    "                    caption_mapping[img_name] = [caption]\n",
    "\n",
    "        for img_name in images_to_skip:\n",
    "            if img_name in caption_mapping:\n",
    "                del caption_mapping[img_name]\n",
    "\n",
    "        return caption_mapping, text_data\n",
    "\n",
    "\n",
    "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
    "    \"\"\"Split the captioning dataset into train and validation sets.\n",
    "\n",
    "    Args:\n",
    "        caption_data (dict): Dictionary containing the mapped caption data\n",
    "        train_size (float): Fraction of all the full dataset to use as training data\n",
    "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
    "\n",
    "    Returns:\n",
    "        Traning and validation datasets as two separated dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Get the list of all image names\n",
    "    all_images = list(caption_data.keys())\n",
    "\n",
    "    # 2. Shuffle if necessary\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "    # 3. Split into training and validation sets\n",
    "    train_size = int(len(caption_data) * train_size)\n",
    "\n",
    "    training_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
    "    }\n",
    "    validation_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
    "    }\n",
    "\n",
    "    # 4. Return the splits\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, valid_data = train_val_split(captions_mapping)\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(valid_data))\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, valid_data = train_val_split(captions_mapping)\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCiJrwHvLKnX"
   },
   "source": [
    "## Vectorizing the text data\n",
    "\n",
    "We'll use the `TextVectorization` layer to vectorize the text data,\n",
    "that is to say, to turn the\n",
    "original strings into integer sequences where each integer represents the index of\n",
    "a word in a vocabulary. We will use a custom string standardization scheme\n",
    "(strip punctuation characters except `<` and `>`) and the default\n",
    "splitting scheme (split on whitespace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eNZgo97zElC"
   },
   "source": [
    "### Using AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJ3haSFMzSrE"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWa0XNPUzBZB"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "0d7ca65713bb4f22b38433794bdfd4e1",
      "eb1dc719724441c1838b09c5a065a7c2",
      "967deecb9b3f42ba9969726c1abb0a95",
      "c1fe7dd230ba4cb898dc34190d581514",
      "bfddf416df7e4f3999a2f4787cfd0dc5",
      "fce3122b3a1743dba8534e872113431a",
      "5e13d9f0698c4e9e8484478e75a49be8",
      "e4522da882154a999ae8328bc77d7add",
      "7fa0dcbd0f514903820fa876f74cc8ca",
      "755c30e265804bd88f0510398ebfc226",
      "8625886877c64fedbb2638aa32571a51",
      "d16698892bf347b7bef5153e452dd2d6",
      "5a8b1ade49fe4264bfb4b0ab5797f4eb",
      "2a26d5675cbc40d692bde6ebbe3f9042",
      "b4ccaf45cb4a471d93e2a7f22929839f",
      "8e7e17f7ea3549d49d7c5a6e50e2124a",
      "c0a424de7b6d4484b9795e0356fc0f5b",
      "3493d6d923494219a39c6f827de75ccb",
      "1de942e7be244644b67203e1aeea66a8",
      "7247c746fbce490e85d0b6acf6cd8237",
      "59d67dd60272433ba777edcb8176e5c9",
      "91ab7b687ea9404a81920f9e28976f78",
      "53e7ffa6105b487fb0446caab83f060a",
      "c403d9ccb9d84581a84cede22fa8301b",
      "e6b24ee799f54805a601c176b13017c4",
      "7ffa94df4b504daaaf08113218cd781d",
      "0de4980199164f9587a69c71f808d46f",
      "d6eb52a7689c4027964445ee98718fb3",
      "198651fc94ce42b6a5bf89ccc7b23623",
      "2b9f10609458402b8e38afd0abe35a7a",
      "b67d07cae33d4eb4972775b431da7064",
      "5c85828da60142a39428c505feedaed2",
      "cbc344f1de1c4fb8ac92e436a3a91452"
     ]
    },
    "id": "VkbaTDKizeXr",
    "outputId": "e1709653-a103-407e-b11b-18293350af83"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7TVIu2iz1iz"
   },
   "outputs": [],
   "source": [
    "train_tokens = tokenizer(text_data, padding = True, truncation= True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVqbEtoe0p8z"
   },
   "source": [
    "### Using Keras TextVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ok89fPI7iSgE",
    "outputId": "c3fc8980-02c6-4d5d-aed3-b71cf831adee"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \"/content/drive/MyDrive/glove200d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVMk9itxrzap"
   },
   "outputs": [],
   "source": [
    "####### Vectorization Method 1 #############\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "strip_chars = strip_chars.replace(\"<\", \"\")\n",
    "strip_chars = strip_chars.replace(\">\", \"\")\n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "vectorizer.adapt(text_data)\n",
    "\n",
    "# Data augmentation for image data\n",
    "# image_augmentation = keras.Sequential(\n",
    "#     [\n",
    "#         layers.RandomFlip(\"horizontal\"),\n",
    "#         layers.RandomRotation(0.2),\n",
    "#         layers.RandomContrast(0.3),\n",
    "#     ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQDQjBA3LKnZ"
   },
   "outputs": [],
   "source": [
    "######## Vectorization Method 2 ###########\n",
    "# Un comment it if you wish to use a custom vocab\n",
    "\n",
    "# def custom_standardization(input_string):\n",
    "#     lowercase = tf.strings.lower(input_string)\n",
    "#     return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "# strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "# strip_chars = strip_chars.replace(\"<\", \"\")  # because we don't want to remove \"<\"  and \">\".\n",
    "# strip_chars = strip_chars.replace(\">\", \"\")\n",
    "\n",
    "\n",
    "# vectorizer = TextVectorization(max_tokens=10000, output_sequence_length=SEQ_LENGTH)\n",
    "# text_ds = tf.data.Dataset.from_tensor_slices(text_data).batch(128)\n",
    "# vectorizer.adapt(text_ds)\n",
    "\n",
    "# voc = vectorizer.get_vocabulary()\n",
    "# word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "# num_tokens = len(voc) + 2\n",
    "# embedding_dim = 200\n",
    "# hits = 0\n",
    "# misses = 0\n",
    "\n",
    "# # Prepare embedding matrix\n",
    "# embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # Words not found in embedding index will be all-zeros.\n",
    "#         # This includes the representation for \"padding\" and \"OOV\"\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "#         hits += 1\n",
    "#     else:\n",
    "#         misses += 1\n",
    "# print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# embedding_layer = Embedding(\n",
    "#     num_tokens,\n",
    "#     embedding_dim,\n",
    "#     embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "#     trainable=False,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Data augmentation for image data\n",
    "# image_augmentation = keras.Sequential(\n",
    "#     [\n",
    "#         layers.RandomFlip(\"horizontal\"),\n",
    "#         layers.RandomRotation(0.2),\n",
    "#         layers.RandomContrast(0.3),\n",
    "#     ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4EQkj7QLKnb"
   },
   "source": [
    "## Building a `tf.data.Dataset` pipeline for training\n",
    "\n",
    "We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n",
    "The pipeline consists of two steps:\n",
    "\n",
    "1. Read the image from the disk\n",
    "2. Tokenize all the five captions corresponding to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVMwhjQoQBBC"
   },
   "outputs": [],
   "source": [
    "# image = tf.zeros([10,10,3])\n",
    "\n",
    "# tf.expand_dims(image, axis=-1)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "IIo6wvAALKnd",
    "outputId": "ce7ed872-08e1-4a37-d4b6-9a6d6e6caf8f"
   },
   "outputs": [],
   "source": [
    "\n",
    "def decode_and_resize(img_path, size=IMAGE_SIZE):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)  # IMAGE_SIZE is (299, 299)\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_train_image(img_path, size=IMAGE_SIZE):\n",
    "    img = decode_and_resize(img_path)\n",
    "    img = image_augmentation(tf.expand_dims(img, 0))[0]\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_valid_image(img_path, size=IMAGE_SIZE):\n",
    "    img = decode_and_resize(img_path)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_dataset(images, captions, split=\"train\"):\n",
    "    if split == \"train\":\n",
    "        img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
    "            read_train_image, num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "    else:\n",
    "        img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
    "            read_valid_image, num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "    cap_dataset = tf.data.Dataset.from_tensor_slices(captions).map(\n",
    "        vectorizer, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((img_dataset, cap_dataset))\n",
    "    dataset = dataset.batch(BATCH_SIZE).shuffle(256).prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Pass the list of images and the list of corresponding captions\n",
    "train_dataset = make_dataset(\n",
    "    list(train_data.keys()), list(train_data.values()), split=\"train\"\n",
    ")\n",
    "\n",
    "valid_dataset = make_dataset(\n",
    "    list(valid_data.keys()), list(valid_data.values()), split=\"valid\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEIlP5NvLKne"
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Our image captioning architecture consists of three models:\n",
    "\n",
    "- [Mode 1: A CNN: used to extract the image features](#1)\n",
    "- [Model 2: A TransformerEncoder:](#2)<br> The extracted image features are then passed to a Transformer\n",
    "                    based encoder that generates a new representation of the inputs\n",
    "- [Model 3: A TransformerDecoder:](#3)<br> This model takes the encoder output and the text data\n",
    "                    (sequences) as inputs and tries to learn to generate the caption.\n",
    "\n",
    "And at last our Captioning Model.\n",
    "- [ImageCaptioningModel](#4)\n",
    "\n",
    "- [Utility Functions](#5)\n",
    "<br>Finally...\n",
    "- [Build Model](#6)\n",
    "- [Model training](#7)\n",
    "- [Prediction](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAJEQI3JTsj0"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "#### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fp3OMMGqSyOn"
   },
   "outputs": [],
   "source": [
    "####### Using EfficientNetB0 #########\n",
    "# Un comment it if you wish to go with EfficientNetB0\n",
    "# def get_cnn_model():\n",
    "#     base_model = efficientnet.EfficientNetB0(\n",
    "#         input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n",
    "#     )\n",
    "#     # We freeze our feature extractor\n",
    "#     base_model.trainable = False\n",
    "#     base_model_out = base_model.output\n",
    "#     base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "#     cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "#     return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wi5wZAwTqvM"
   },
   "outputs": [],
   "source": [
    "########### Using ResNet50V2 #########\n",
    "def get_cnn_model():\n",
    "\n",
    "    base_model = tf.keras.applications.ResNet50V2(\n",
    "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n",
    "    )\n",
    "    # We freeze our feature extractor\n",
    "    base_model.trainable = False\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    return cnn_model\n",
    "# get_cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ftxk2vNWTwWQ"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "#### TransformerEncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0mf63uAUM_p"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        inputs = self.layernorm_1(inputs)\n",
    "        inputs = self.dense_1(inputs)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "        return out_1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_tokens = embedded_tokens * self.embed_scale\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN5IewSJUPON"
   },
   "source": [
    "<a name = \"3\"></a>\n",
    "#### TransformerDecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZuHGow_UlPq"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.embedding = PositionalEmbedding(\n",
    "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
    "        )\n",
    "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = layers.Dropout(0.3)\n",
    "        self.dropout_2 = layers.Dropout(0.5)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daUGFDpkVu3W"
   },
   "source": [
    "<a name = \"4\"></a>\n",
    "#### ImageCaptioningModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmK0wEG_Va96"
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(keras.Model):\n",
    "    def __init__(\n",
    "      self, cnn_model, encoder, decoder, num_captions_per_image=5,):\n",
    "      super().__init__()\n",
    "      self.cnn_model = cnn_model\n",
    "      self.encoder = encoder\n",
    "      self.decoder = decoder\n",
    "      self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "      self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "      self.num_captions_per_image = num_captions_per_image\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "      loss = self.loss(y_true, y_pred)\n",
    "      mask = tf.cast(mask, dtype=loss.dtype)\n",
    "      loss *= mask\n",
    "      return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "      accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "      accuracy = tf.math.logical_and(mask, accuracy)\n",
    "      accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "      mask = tf.cast(mask, dtype=tf.float32)\n",
    "      return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "      encoder_out = self.encoder(img_embed, training=training)\n",
    "      batch_seq_inp = batch_seq[:, :-1]\n",
    "      batch_seq_true = batch_seq[:, 1:]\n",
    "      mask = tf.math.not_equal(batch_seq_true, 0)\n",
    "      batch_seq_pred = self.decoder(\n",
    "          batch_seq_inp, encoder_out, training=training, mask=mask\n",
    "      )\n",
    "      loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "      acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "      return loss, acc\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "      batch_img, batch_seq = batch_data\n",
    "      batch_loss = 0\n",
    "      batch_acc = 0\n",
    "\n",
    "      # 1. Get image embeddings\n",
    "      img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "      # 2. Pass each of the five captions one by one to the decoder\n",
    "      # along with the encoder outputs and compute the loss as well as accuracy\n",
    "      # for each caption.\n",
    "      for i in range(self.num_captions_per_image):\n",
    "          with tf.GradientTape() as tape:\n",
    "              loss, acc = self._compute_caption_loss_and_acc(\n",
    "                  img_embed, batch_seq[:, i, :], training=True\n",
    "              )\n",
    "\n",
    "              # 3. Update loss and accuracy\n",
    "              batch_loss += loss\n",
    "              batch_acc += acc\n",
    "\n",
    "          # 4. Get the list of all the trainable weights\n",
    "          train_vars = (\n",
    "              self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "          )\n",
    "\n",
    "          # 5. Get the gradients\n",
    "          grads = tape.gradient(loss, train_vars)\n",
    "\n",
    "          # 6. Update the trainable weights\n",
    "          self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "      # 7. Update the trackers\n",
    "      batch_acc /= float(self.num_captions_per_image)\n",
    "      self.loss_tracker.update_state(batch_loss)\n",
    "      self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "      # 8. Return the loss and accuracy values\n",
    "      return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "      batch_img, batch_seq = batch_data\n",
    "      batch_loss = 0\n",
    "      batch_acc = 0\n",
    "\n",
    "      # 1. Get image embeddings\n",
    "      img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "      # 2. Pass each of the five captions one by one to the decoder\n",
    "      # along with the encoder outputs and compute the loss as well as accuracy\n",
    "      # for each caption.\n",
    "      for i in range(self.num_captions_per_image):\n",
    "          loss, acc = self._compute_caption_loss_and_acc(\n",
    "              img_embed, batch_seq[:, i, :], training=False\n",
    "          )\n",
    "\n",
    "          # 3. Update batch loss and batch accuracy\n",
    "          batch_loss += loss\n",
    "          batch_acc += acc\n",
    "\n",
    "      batch_acc /= float(self.num_captions_per_image)\n",
    "\n",
    "      # 4. Update the trackers\n",
    "      self.loss_tracker.update_state(batch_loss)\n",
    "      self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "      # 5. Return the loss and accuracy values\n",
    "      return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "      # We need to list our metrics here so the `reset_states()` can be\n",
    "      # called automatically.\n",
    "      return [self.loss_tracker, self.acc_tracker]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQ9tZYLkWNhM"
   },
   "source": [
    "<a name = \"5\"></a>\n",
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdDihvbv16ne"
   },
   "source": [
    "### Using pretrained transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s15klAhpLKne"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124,
     "referenced_widgets": [
      "72acb86405914480b0e4f41169e3036d",
      "2c0647217837470b82ed1798a9329c2f",
      "c8e4b00fbe964e988f21db3bb567cc58",
      "52038c0798cc4a9390b43b1d2cf59d48",
      "bd0ea64fb7df4ad49aa84bc40d886276",
      "4b7f8d2387f941bba44a7ab1edb303e6",
      "a154ad08d11f408c9b00beaf0af0f19a",
      "76ed6ca1e2a840148a9a580363ffb4ca",
      "d85e4e5af6ad4a9ba217a03dfb718c2a",
      "77edcfb00cff40e4bbc27afd1d41d78e",
      "ecc407a626374b9c92b6aea0ee1a2c2d"
     ]
    },
    "id": "NfnP33YS2G8O",
    "outputId": "03661f95-7f22-4223-f764-f86a6369b343"
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNy8f-fh2G_O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73lnRXASWFih"
   },
   "source": [
    "<a name = \"6\"></a>\n",
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ma29IY5WEuk"
   },
   "outputs": [],
   "source": [
    "cnn_model = get_cnn_model()\n",
    "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=4)\n",
    "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=8)\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcW1WrYVLKnh"
   },
   "source": [
    "<a name = \"7\"></a>\n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LhDOmqyLKni"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the loss function\n",
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# EarlyStopping criteria\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler for the optimizer\n",
    "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "    super().__init__()\n",
    "    self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    global_step = tf.cast(step, tf.float32)\n",
    "    warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "    warmup_progress = global_step / warmup_steps\n",
    "    warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "    return tf.cond(\n",
    "        global_step < warmup_steps,\n",
    "        lambda: warmup_learning_rate,\n",
    "        lambda: self.post_warmup_learning_rate,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a learning rate schedule\n",
    "num_train_steps = len(train_dataset) * EPOCHS\n",
    "num_warmup_steps = num_train_steps // 15\n",
    "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3s6JJgCmt9A"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWyxR0Q-UPS0"
   },
   "outputs": [],
   "source": [
    "model_path = \"/content/drive/MyDrive/Image Captioning/caption_model.h5\"\n",
    "# model = keras.models.load_model(model_path, custom_objects={\"CustomModel\": caption_model})\n",
    "# caption_model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cXJte90oUo4"
   },
   "outputs": [],
   "source": [
    "# caption_model.build()\n",
    "# caption_model.load_weights(\"/content/drive/MyDrive/Image Captioning/caption_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1SD1YdcmqHv",
    "outputId": "4a56650d-19f1-4953-e964-6a141128ac14"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "caption_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tz-OWKIsLn-"
   },
   "outputs": [],
   "source": [
    "caption_model.save_weights(\"caption_model_16heads.h5\")\n",
    "# with open(\"caption_model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# model.save_weights(\"caption_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmKqMFgzuY13"
   },
   "outputs": [],
   "source": [
    "caption_model.save_weights(\"/content/drive/MyDrive/Image Captioning/model.pb\" ,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSd-1t6kuZuV"
   },
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model('/content/drive/MyDrive/Image Captioning/checkpoint.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "CtOnH99rukk5",
    "outputId": "94b3ab8c-9d69-489c-c520-17df8f5d14a1"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy(\"/content/caption_model.h5\",\"/content/drive/MyDrive/Image Captioning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2yzh3tTLKnj"
   },
   "source": [
    "<a name = \"8\"></a>\n",
    "## Check sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "17eoyWJtLKnk",
    "outputId": "00618199-01e4-47fb-a334-e0f8411d7c93"
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
    "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
    "valid_images = list(valid_data.keys())\n",
    "\n",
    "\n",
    "def generate_caption():\n",
    "    # Select a random image from the validation dataset\n",
    "    # sample_img = np.random.choice(valid_images)\n",
    "    sample_img = \"/content/Flicker8k_Dataset/1007129816_e794419615.jpg\"\n",
    "\n",
    "    # Read the image from the disk\n",
    "    sample_img = read_valid_image(sample_img)\n",
    "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    # Pass the image to the CNN\n",
    "    img = tf.expand_dims(sample_img, 0)\n",
    "    img = caption_model.cnn_model(img)\n",
    "\n",
    "    # Pass the image features to the Transformer encoder\n",
    "    encoded_img = caption_model.encoder(img, training=False)\n",
    "\n",
    "    # Generate the caption using the Transformer decoder\n",
    "    decoded_caption = \"<start> \"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_caption = vectorizer([decoded_caption])[:, :-1]\n",
    "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
    "        predictions = caption_model.decoder(\n",
    "            tokenized_caption, encoded_img, training=False, mask=mask\n",
    "        )\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = index_lookup[sampled_token_index]\n",
    "        if sampled_token == \" <end>\":\n",
    "            break\n",
    "        decoded_caption += \" \" + sampled_token\n",
    "\n",
    "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
    "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
    "    print(\"Predicted Caption: \", decoded_caption)\n",
    "\n",
    "\n",
    "# Check predictions for a few samples\n",
    "generate_caption()\n",
    "generate_caption()\n",
    "generate_caption()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLgcC8u_LKnl"
   },
   "source": [
    "## End Notes\n",
    "\n",
    "We saw that the model starts to generate reasonable captions after a few epochs. To keep\n",
    "this example easily runnable, we have trained it with a few constraints, like a minimal\n",
    "number of attention heads. To improve the predictions, you can try changing these training\n",
    "settings and find a good model for your use case."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d7ca65713bb4f22b38433794bdfd4e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_967deecb9b3f42ba9969726c1abb0a95",
       "IPY_MODEL_c1fe7dd230ba4cb898dc34190d581514",
       "IPY_MODEL_bfddf416df7e4f3999a2f4787cfd0dc5"
      ],
      "layout": "IPY_MODEL_eb1dc719724441c1838b09c5a065a7c2"
     }
    },
    "0de4980199164f9587a69c71f808d46f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbc344f1de1c4fb8ac92e436a3a91452",
      "placeholder": "​",
      "style": "IPY_MODEL_5c85828da60142a39428c505feedaed2",
      "value": " 226k/226k [00:00&lt;00:00, 822kB/s]"
     }
    },
    "198651fc94ce42b6a5bf89ccc7b23623": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1de942e7be244644b67203e1aeea66a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a26d5675cbc40d692bde6ebbe3f9042": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3493d6d923494219a39c6f827de75ccb",
      "placeholder": "​",
      "style": "IPY_MODEL_c0a424de7b6d4484b9795e0356fc0f5b",
      "value": "Downloading: 100%"
     }
    },
    "2b9f10609458402b8e38afd0abe35a7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c0647217837470b82ed1798a9329c2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3493d6d923494219a39c6f827de75ccb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b7f8d2387f941bba44a7ab1edb303e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52038c0798cc4a9390b43b1d2cf59d48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d85e4e5af6ad4a9ba217a03dfb718c2a",
      "max": 267844284,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76ed6ca1e2a840148a9a580363ffb4ca",
      "value": 267844284
     }
    },
    "53e7ffa6105b487fb0446caab83f060a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6b24ee799f54805a601c176b13017c4",
       "IPY_MODEL_7ffa94df4b504daaaf08113218cd781d",
       "IPY_MODEL_0de4980199164f9587a69c71f808d46f"
      ],
      "layout": "IPY_MODEL_c403d9ccb9d84581a84cede22fa8301b"
     }
    },
    "59d67dd60272433ba777edcb8176e5c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a8b1ade49fe4264bfb4b0ab5797f4eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c85828da60142a39428c505feedaed2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e13d9f0698c4e9e8484478e75a49be8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7247c746fbce490e85d0b6acf6cd8237": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72acb86405914480b0e4f41169e3036d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8e4b00fbe964e988f21db3bb567cc58",
       "IPY_MODEL_52038c0798cc4a9390b43b1d2cf59d48",
       "IPY_MODEL_bd0ea64fb7df4ad49aa84bc40d886276"
      ],
      "layout": "IPY_MODEL_2c0647217837470b82ed1798a9329c2f"
     }
    },
    "755c30e265804bd88f0510398ebfc226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76ed6ca1e2a840148a9a580363ffb4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "77edcfb00cff40e4bbc27afd1d41d78e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fa0dcbd0f514903820fa876f74cc8ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ffa94df4b504daaaf08113218cd781d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b67d07cae33d4eb4972775b431da7064",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b9f10609458402b8e38afd0abe35a7a",
      "value": 231508
     }
    },
    "8625886877c64fedbb2638aa32571a51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e7e17f7ea3549d49d7c5a6e50e2124a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91ab7b687ea9404a81920f9e28976f78",
      "placeholder": "​",
      "style": "IPY_MODEL_59d67dd60272433ba777edcb8176e5c9",
      "value": " 629/629 [00:00&lt;00:00, 14.1kB/s]"
     }
    },
    "91ab7b687ea9404a81920f9e28976f78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "967deecb9b3f42ba9969726c1abb0a95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e13d9f0698c4e9e8484478e75a49be8",
      "placeholder": "​",
      "style": "IPY_MODEL_fce3122b3a1743dba8534e872113431a",
      "value": "Downloading: 100%"
     }
    },
    "a154ad08d11f408c9b00beaf0af0f19a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4ccaf45cb4a471d93e2a7f22929839f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7247c746fbce490e85d0b6acf6cd8237",
      "max": 629,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1de942e7be244644b67203e1aeea66a8",
      "value": 629
     }
    },
    "b67d07cae33d4eb4972775b431da7064": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd0ea64fb7df4ad49aa84bc40d886276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecc407a626374b9c92b6aea0ee1a2c2d",
      "placeholder": "​",
      "style": "IPY_MODEL_77edcfb00cff40e4bbc27afd1d41d78e",
      "value": " 255M/255M [00:09&lt;00:00, 30.5MB/s]"
     }
    },
    "bfddf416df7e4f3999a2f4787cfd0dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8625886877c64fedbb2638aa32571a51",
      "placeholder": "​",
      "style": "IPY_MODEL_755c30e265804bd88f0510398ebfc226",
      "value": " 48.0/48.0 [00:00&lt;00:00, 1.06kB/s]"
     }
    },
    "c0a424de7b6d4484b9795e0356fc0f5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1fe7dd230ba4cb898dc34190d581514": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fa0dcbd0f514903820fa876f74cc8ca",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4522da882154a999ae8328bc77d7add",
      "value": 48
     }
    },
    "c403d9ccb9d84581a84cede22fa8301b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8e4b00fbe964e988f21db3bb567cc58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a154ad08d11f408c9b00beaf0af0f19a",
      "placeholder": "​",
      "style": "IPY_MODEL_4b7f8d2387f941bba44a7ab1edb303e6",
      "value": "Downloading: 100%"
     }
    },
    "cbc344f1de1c4fb8ac92e436a3a91452": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d16698892bf347b7bef5153e452dd2d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a26d5675cbc40d692bde6ebbe3f9042",
       "IPY_MODEL_b4ccaf45cb4a471d93e2a7f22929839f",
       "IPY_MODEL_8e7e17f7ea3549d49d7c5a6e50e2124a"
      ],
      "layout": "IPY_MODEL_5a8b1ade49fe4264bfb4b0ab5797f4eb"
     }
    },
    "d6eb52a7689c4027964445ee98718fb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d85e4e5af6ad4a9ba217a03dfb718c2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4522da882154a999ae8328bc77d7add": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6b24ee799f54805a601c176b13017c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_198651fc94ce42b6a5bf89ccc7b23623",
      "placeholder": "​",
      "style": "IPY_MODEL_d6eb52a7689c4027964445ee98718fb3",
      "value": "Downloading: 100%"
     }
    },
    "eb1dc719724441c1838b09c5a065a7c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecc407a626374b9c92b6aea0ee1a2c2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fce3122b3a1743dba8534e872113431a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
