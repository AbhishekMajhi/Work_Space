{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Embeddings With BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6BL6f0J7ziEelduTZNTdv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhishekMajhi/DeepLearning-Using-Tensorflow/blob/master/RNN/Sentence_Embeddings_With_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWnjxbh43SuI"
      },
      "source": [
        "#!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_t1KR253hgn",
        "outputId": "e5fe0a81-9f40-4699-8160-45e1d8882dfc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(48)\n",
        "# Import libraries\n",
        "\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Lambda,Dense,BatchNormalization, Flatten, Dropout, Activation\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.initializers import glorot_uniform\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-20 04:28:43.407324: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYWPPovl57-f"
      },
      "source": [
        "import csv\n",
        "def read_csv(filename = '/content/train_command.csv'):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PndIqgAs6BSk"
      },
      "source": [
        "X_train, Y_train = read_csv('/content/train_command.csv')\n",
        "X_test, Y_test= read_csv('/content/validation command.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFdxz8PFA6r_",
        "outputId": "3bae6e90-4b08-4dcc-8b88-50425f6f93f2"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPouVbpy6zzq"
      },
      "source": [
        "## BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycMujP8T6xKu"
      },
      "source": [
        "from simpletransformers.language_representation import RepresentationModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHD-nyJf7ZuA",
        "outputId": "70a2cbaa-ea53-47db-db97-23085d7256a8"
      },
      "source": [
        "# Initializing BERT Model\n",
        "model_bert = RepresentationModel(model_type= 'bert',\n",
        "                            model_name= 'bert-base-uncased',\n",
        "                            use_cuda= False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTextRepresentation: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIQNAZcY_U3a"
      },
      "source": [
        "train_sentence_vec = model_bert.encode_sentences(X_train,combine_strategy=\"mean\")\n",
        "val_sentence_vec = model_bert.encode_sentences(X_test,combine_strategy=\"mean\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq4972Xn7eWv",
        "outputId": "f393b3e6-5f70-4a30-dfc5-84c123b5e082"
      },
      "source": [
        "print(\"train_vec:\",train_sentence_vec.shape)\n",
        "print(\"val_vec:\",val_sentence_vec.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_vec: (167, 768)\n",
            "val_vec: (43, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz-vyXxm-DKz",
        "outputId": "c954fb26-9bc0-4780-bc79-13313f4c97c2"
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())\n",
        "maxLen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yWjB49NCB-h"
      },
      "source": [
        "# one-hot encode\n",
        "Y_train_oh = tf.keras.utils.to_categorical(Y_train,num_classes = 23)\n",
        "Y_test_oh = tf.keras.utils.to_categorical(Y_test,num_classes = 23)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNgSdix-Gdvw",
        "outputId": "764d034c-5a76-4cd2-afcc-42dd911f6c2a"
      },
      "source": [
        "train_sentence_vec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BDkxX3OGNde",
        "outputId": "420fb344-8e1e-436d-e7f5-acbc510dc51b"
      },
      "source": [
        "Y_train_oh.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o6JgjST-Og5"
      },
      "source": [
        "## Similarity Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16z6YLS2-hDq"
      },
      "source": [
        "### temp code for exper..###\n",
        "def similarity_model(input_shape):\n",
        "    \"\"\"\n",
        "    Function creating the Emojify-v2 model's graph.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the input, usually (max_len,)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    model -- a model instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    # Define sentence_indices as the input of the graph.\n",
        "    # It should be of shape input_shape and dtype 'int32' (as it contains indices, which are integers).\n",
        "    embeddings = tf.keras.layers.Input(input_shape,dtype = 'float32')\n",
        "    \n",
        "    # # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
        "    # embedding_layer = pretrained_embedding_layer(word_to_vec_map,word_to_index)\n",
        "    \n",
        "    # # Propagate sentence_indices through your embedding layer\n",
        "    # # (See additional hints in the instructions).\n",
        "    # embeddings = embedding_layer(sentence_indices)   \n",
        "    \n",
        "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a batch of sequences.\n",
        "    X = tf.keras.layers.LSTM(128,return_sequences =True, name = 'hidden1')(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = tf.keras.layers.Dropout(0.5)(X)\n",
        "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = tf.keras.layers.LSTM(128,return_sequences = True)(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = tf.keras.layers.Dropout(0.5)(X)\n",
        "    # X = LSTM(128,return_sequences = True,activity_regularizer= l2(0.03))(X)\n",
        "    # X = BatchNormalization()(X)\n",
        "    # X = Dropout(0.5)(X)\n",
        "    X = tf.keras.layers.LSTM(64,return_sequences = False)(X)\n",
        "    #X = Flatten()(X)\n",
        "    # Propagate X through a Dense layer with 5 units\n",
        "    X = tf.keras.layers.Dense(units=23)(X)\n",
        "    # Add a softmax activation\n",
        "    X = tf.keras.layers.Activation('softmax')(X)\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = tf.keras.models.Model(embeddings,X)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP9fg3I9H3Ic",
        "outputId": "27840b2c-f825-4d37-add1-9f5a0bdd0497"
      },
      "source": [
        "train_sentence_vec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbVw62l1H7kd",
        "outputId": "f98b0c96-1175-41e8-cc7e-822e570e559d"
      },
      "source": [
        "Y_train_oh.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIk4lMfpDqVz",
        "outputId": "f0f9b616-2e8a-443f-9ce5-ab6072bac1ad"
      },
      "source": [
        "model = similarity_model((train_sentence_vec.shape[1],1))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 768, 1)]          0         \n",
            "_________________________________________________________________\n",
            "hidden1 (LSTM)               (None, 768, 128)          66560     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 768, 128)          512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 768, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 768, 128)          131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 768, 128)          512       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 768, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 23)                1495      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 23)                0         \n",
            "=================================================================\n",
            "Total params: 250,071\n",
            "Trainable params: 249,559\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "076CMH7_DqYR"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfsLXjRVKVyb",
        "outputId": "a528b160-7226-42b0-e298-4b413e762b36"
      },
      "source": [
        "print(train_sentence_vec.shape)\n",
        "print(Y_train_oh.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(167, 768)\n",
            "(167, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySVogX0IDqb4"
      },
      "source": [
        "history = model.fit(train_sentence_vec, Y_train_oh, epochs = 1000, batch_size = 30, shuffle=True,validation_data=(val_sentence_vec, Y_test_oh))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB88tNFkGA1P"
      },
      "source": [
        "sen = np.array(['what will be the weather today?'])\n",
        "emb = model_bert.encode_sentences(sen,combine_strategy=\"mean\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1HTnltqOXwo",
        "outputId": "32ed8576-699d-4c41-b502-ceadc0911126"
      },
      "source": [
        "print(np.argmax(model.predict(emb)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkDLkeOdOhWQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}